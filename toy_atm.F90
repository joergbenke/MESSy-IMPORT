! Copyright (c) 2024 The YAC Authors
!
! SPDX-License-Identifier: BSD-3-Clause

MODULE toy_atm

  USE mpi
  USE yac
  USE toy_common, ONLY : read_icon_grid, nsteps, define_fields, &
                         send_field, receive_field, &
                         max_char_length

  IMPLICIT NONE

  PRIVATE

  INTEGER :: t, ierror

  ! Basic string paramters
  CHARACTER(LEN=max_char_length), PARAMETER :: yaml_filename = "input/coupling.yaml"
  CHARACTER(LEN=max_char_length), PARAMETER :: grid_filename = "grids/icon_grid_0030_R02B03_G.nc"
  CHARACTER(LEN=max_char_length), PARAMETER :: comp_name = "atm_comp"
  CHARACTER(LEN=max_char_length), PARAMETER :: grid_name = "atm_grid"

  ! IDs and communicator generated by YAC
  INTEGER :: comp_id
  INTEGER :: comp_rank
  INTEGER :: comp_comm
  INTEGER :: grid_id
  INTEGER :: cell_point_id
  INTEGER :: field_taux_id
  INTEGER :: field_tauy_id
  INTEGER :: field_sfwflx_id
  INTEGER :: field_sftemp_id
  INTEGER :: field_thflx_id
  INTEGER :: field_iceatm_id
  INTEGER :: field_sst_id
  INTEGER :: field_oceanu_id
  INTEGER :: field_oceanv_id
  INTEGER :: field_iceoce_id

  ! Basic decomposed grid information
  INTEGER                       :: num_vertices
  INTEGER                       :: num_cells
  INTEGER                       :: num_vertices_per_cell
  INTEGER, ALLOCATABLE          :: cell_to_vertex(:,:)
  DOUBLE PRECISION, ALLOCATABLE :: x_vertices(:)
  DOUBLE PRECISION, ALLOCATABLE :: y_vertices(:)
  DOUBLE PRECISION, ALLOCATABLE :: x_cells(:)
  DOUBLE PRECISION, ALLOCATABLE :: y_cells(:)
  INTEGER, ALLOCATABLE          :: cell_sea_land_mask(:)
  INTEGER, ALLOCATABLE          :: global_cell_id(:)

  ! Buffer for field data
  DOUBLE PRECISION, DIMENSION(:,:), ALLOCATABLE :: &
       taux, tauy, sfwflx, sftemp, thflx, iceatm, sst, oceanu, oceanv, iceoce

PUBLIC :: main_atm

CONTAINS

  SUBROUTINE main_atm(comm)

    integer(kind = 4) :: n_nodes, n_elements
    INTEGER, INTENT(IN) :: comm
    character (len = 200) :: file = 'grids/GEIA_MPIC1.0_X_bioland_NH3_2000-2000.nc' 
    
    comp_comm = comm


    ! Read coupling configuration file
    CALL yac_fread_config_yaml(yaml_filename)

    ! Define local component
    CALL yac_fdef_comp(comp_name, comp_id)

    ! Retrieve communicator for ATM component
    CALL yac_fget_comp_comm(comp_id, comp_comm)

    CALL MPI_Comm_rank(comp_comm, comp_rank, ierror)

    ! Read the grid and distribute it among all ATM processes
    CALL read_icon_grid( &
         grid_filename, comp_comm, cell_to_vertex, x_vertices, &
         y_vertices, x_cells, y_cells, cell_sea_land_mask, &
         global_cell_id)
    num_vertices = SIZE(x_vertices)
    num_cells = SIZE(x_cells)
    num_vertices_per_cell = SIZE(cell_to_vertex, 1)

    call read_grid_from_netcdf(trim(file))

    
    ! Define local part of the grid
    CALL yac_fdef_grid ( &
         grid_name, num_vertices, num_cells, num_vertices_per_cell, &
         x_vertices, y_vertices, cell_to_vertex, grid_id )

    ! Set global cell ids
    CALL yac_fset_global_index(global_cell_id, YAC_LOCATION_CELL, grid_id)

    ! Define location of the actual data (on cell centers)
    CALL yac_fdef_points ( &
         grid_id, num_cells, YAC_LOCATION_CELL, &
         x_cells, y_cells, cell_point_id )

    !   ! Set mask for cell centers
    !   CALL yac_fset_mask(cell_sea_land_mask >= 0, cell_point_id)

    ! Define fields
    CALL define_fields( &
         comp_id, cell_point_id, field_taux_id, field_tauy_id, field_sfwflx_id, &
         field_sftemp_id, field_thflx_id, field_iceatm_id, field_sst_id, &
         field_oceanu_id, field_oceanv_id, field_iceoce_id)

    ! Complete definitions and compute interpolations
    CALL yac_fenddef()

    ! Initialise fields
    CALL init_fields()

    ! Execute model time loop
    DO t = 1, nsteps

       ! Simulate atm timestep
       CALL sim_atm_timestep(t)

       ! Exchange data with ocn component
       CALL couple_to_ocn()

    END DO ! time loop

  END SUBROUTINE main_atm

  SUBROUTINE init_fields()

    ! Allocate output field buffers
    ALLOCATE( &
      taux(num_cells, 2), tauy(num_cells, 2), sfwflx(num_cells, 3), &
      sftemp(num_cells, 1), thflx(num_cells, 4), iceatm(num_cells, 4))

    ! Initialise output field buffer with dummy data
    taux(:,1) = 10.1d0
    taux(:,2) = 10.2d0
    tauy(:,1) = 20.1d0
    tauy(:,2) = 20.2d0
    sfwflx(:,1) = 30.1d0
    sfwflx(:,2) = 30.2d0
    sfwflx(:,3) = 30.3d0
    sftemp(:,1) = 40.1d0
    thflx(:,1) = 50.1d0
    thflx(:,2) = 50.2d0
    thflx(:,3) = 50.3d0
    thflx(:,4) = 50.4d0
    iceatm(:,1) = 60.1d0
    iceatm(:,2) = 60.2d0
    iceatm(:,3) = 60.3d0
    iceatm(:,4) = 60.4d0

    ! Allocate input field buffers
    ALLOCATE( &
      sst(num_cells, 1), oceanu(num_cells, 1), oceanv(num_cells, 1), &
      iceoce(num_cells, 5))

    ! Initialise input field buffer with zero
    sst(:,1) = 0.0d0
    oceanu(:,1) = 0.0d0
    oceanv(:,1) = 0.0d0
    iceoce(:,1) = 0.0d0
    iceoce(:,2) = 0.0d0
    iceoce(:,3) = 0.0d0
    iceoce(:,4) = 0.0d0
    iceoce(:,5) = 0.0d0

  END SUBROUTINE init_fields

  SUBROUTINE sim_atm_timestep(timestep)

    INTEGER, INTENT(IN) :: timestep

    ! Do an atmosphere timestep

    ! ...

    CALL MPI_Barrier(comp_comm, ierror)
    IF (comp_rank == 0) &
      PRINT "('--- ',A3,' timestep ',I2,' ---')", comp_name, timestep

  END SUBROUTINE sim_atm_timestep

  SUBROUTINE couple_to_ocn()

    ! --------------------
    ! Send fields to ocean
    ! --------------------

    ! Send meridional wind stress
    CALL send_field(field_taux_id, taux)

    ! Send zonal  wind stress
    CALL send_field(field_tauy_id, tauy)

    ! Send surface fresh water flux
    CALL send_field(field_sfwflx_id, sfwflx)

    ! Send surface temperature
    CALL send_field(field_sftemp_id, sftemp)

    ! Send total heat flux
    CALL send_field(field_thflx_id, thflx)

    ! Send ice temperatures and melt potential
    CALL send_field(field_iceatm_id, iceatm)

    ! -------------------------
    ! Receive fields from ocean
    ! -------------------------

    ! Receive sea surface temperature
    CALL receive_field(comp_name, field_sst_id, sst)

    ! Recieve zonal velocity
    CALL receive_field(comp_name, field_oceanu_id, oceanu)

    ! Receive meridional velocity
    CALL receive_field(comp_name, field_oceanv_id, oceanv)

    ! Receive Ice thickness, concentration, T1 and T2
    CALL receive_field(comp_name, field_iceoce_id, iceoce)

  END SUBROUTINE couple_to_ocn

! ===================== subroutine parse =========================
subroutine read_grid_from_netcdf(filename)
  use netcdf
  implicit none

  character(len = 200) :: filename
  character(len = 200) :: name
  character(len = 30), allocatable, dimension(:) :: dimnames

  integer :: status, xtype, ncid, ndim, nvar, natt, natts
  integer :: len, k, ndims, l1, l2, l3, k_un
  integer :: validRangeLength
  integer, allocatable, dimension(:) :: dimids, dimlengths

  real, allocatable, dimension(:,:,:) :: data

  ! Open netCDF file
  status = nf90_open(trim(filename), nf90_nowrite, ncid)
  if (status /= nf90_noerr) then
     write(*, *) 'could not open::', filename
     write(*, *) status
     stop 'parse_nc [1]'
  endif

  ! Inquiry of the nuber of variables, etc
  write(*, *)
  write(*, *) "----- Inquire number of dimensions, variables, etc -----"
  status = nf90_inquire( ncid, ndim, nvar, natt, k_un )
  if (status /= nf90_noerr) then
     write(*, *) 'nf90_inquire error'
     write(*, *) status
     stop 'parse_nc [1]'
  endif
  write(*, *) 'ndim, nvar, natt, k_un:', ndim, nvar, natt, k_un

  ! Output of the dimensions
  allocate(dimids(ndim), dimlengths(ndim), dimnames(ndim))

  write(*, *)
  write(*, *) "----- Dimensions -----"
  do k = 1, ndim
     status = nf90_inquire_dimension(ncid, k, name, len)
     if (status /= nf90_noerr) then
       write(*, *) "n90_inquire_dimension error"
       write(*, *) status
       stop 'parse_nc [1]'
    endif
    
     dimlengths(k) = len
     dimnames(k) = name
     write(*, *) 'axes ', k, name, len
  enddo

  ! Output of the variables
  write(*, *)
  write(*, *) "----- Variables -----"
  do k = 1, nvar
     name = ''
     status = nf90_inquire_variable(ncid, k, name, xtype, ndims, dimids, natts)
     if (status /= nf90_noerr) then
       write(*, *) "n90_inquire_variable error"
       write(*, *) status
       stop 'parse_nc [1]'
     endif

     write(*, '(a, i3, " ", a, " ",i5, 2h::, 5i4)') 'variable ', k, trim(name(1 : 200)), ndims, dimids(1 : ndims), xtype, natts

!     status = nf90_inquire_attribute(ncid, k, 'lon', len = validrangelength)
!N     if (status /= nf90_noerr) then
!       write(*, *) "n90_inquire_attribute error"
!       write(*, *) status
!       stop 'parse_nc [1]'
!     endif

!     if ( name(1:3) == 'lon' ) then
!        l1 = dimlengths(dimids(1))
!        l2 = dimlengths(dimids(2))
!        l3 = dimlengths(dimids(3))
!        allocate( data(l1, l2, l3) )
!        status = nf90_get_var( ncid, k, data, (/ 1,1,1 /) )
        
        if (natts > 0) then
           write(* ,*) '==== data attributes ===='
           call parse_atlist( ncid, natts, k )
        endif
 !    endif
  enddo

  ! Output of the attributes
  write(*, *) "----- Attributes -----"
  if(natt > 0) then
     write(*, *) '==== global attributes ===='
     call parse_atlist(ncid, natt, nf90_global)
  endif

end subroutine read_grid_from_netcdf

!=============================================================================
subroutine parse_atlist( ncid, natt, varid )
  use netcdf
  
  character (len = 256) :: c1d
  character (len = 128) :: name
  integer :: ncid, natt, varid
  integer :: xtype, len
  integer :: k, kk, status
  real :: rval

  integer, allocatable, dimension(:) :: ivals
  real, allocatable, dimension(:) :: rvals

  do k = 1, natt
     status = nf90_inq_attname(ncid, varid, k, name)
     status = nf90_inquire_attribute(ncid, varid, name, xtype, len, kk)

     if (xtype == NF90_CHAR) then
        if (len > 256) then
           write(*,*) 'truncating attribute to length:',256
        endif
        status = nf90_get_att(ncid, varid, name, c1d )
        if (status.ne.0) stop 'error calling get_att'
        write(*, '(a," ", a)') trim(name), trim(c1d)
     else if ( xtype.eq.NF90_INT ) then
        if (len.gt.1) then
           allocate( ivals(len) )
           status = nf90_get_att(ncid, varid, name, ivals )
           if (status.ne.0) stop &
                'error calling get_att for integer array'
           write(*,'(a,3h:: ,(i6))') trim(name), ivals
           deallocate( ivals )
        else
           status = nf90_get_att(ncid, varid, name, ivals )
           if (status.ne.0) stop 'error calling get_att for integer'
           write(6,'(a,3h:: ,(i6))') trim(name), ivals
        endif
     else if( xtype.eq.NF90_FLOAT ) then
        if (len.gt.1) then
           allocate( rvals(len) )
           status = nf90_get_att(ncid, varid, name, rvals )
           if (status.ne.0) stop 'error calling get_att for real array'
           write(*,'(a,3h:: ,(e12.5))') trim(name), rvals
           deallocate( rvals )
        else
           status = nf90_get_att(ncid, varid, name, rval )
           if(status.ne.0) stop 'error calling get_att for real'
           write(6,'(a,3h:: ,(e12.5))') trim(name), rval
        endif
     else if( xtype.eq.NF90_DOUBLE ) then
!        write(6,*)  'WARNING:: not programmed for double global attribute'
        if (len.gt.1) then
           allocate( rvals(len) )
           status = nf90_get_att(ncid, varid, name, rvals )
           if (status.ne.0) stop 'error calling get_att for real array'
           write(*,'(a,3h:: ,(e12.5))') trim(name), rvals
           deallocate( rvals )
        else
           status = nf90_get_att(ncid, varid, name, rval )
           if(status .ne. 0) stop 'error calling get_att for real'
           write(6,'(a,3h:: ,(e12.5))') trim(name), rval
        endif

     endif
  enddo

end subroutine parse_atlist
  
END MODULE toy_atm


PROGRAM main_atm_program
  USE mpi
  USE yac
  USE netcdf
  USE toy_atm, ONLY : main_atm

  IMPLICIT NONE

  INTEGER :: ierror

  ! Initialise MPI
  CALL MPI_Init(ierror)

  ! Initialise the YAC
  CALL yac_finit()

  ! Run atmosphere model
  CALL main_atm(MPI_COMM_WORLD)

  ! Finalise YAC
  CALL yac_ffinalize()

  ! Finalise MPI
  CALL MPI_Finalize(ierror)

END PROGRAM main_atm_program
