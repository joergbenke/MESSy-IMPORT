! Copyright (c) 2024 The YAC Authors
!
! SPDX-License-Identifier: BSD-3-Clause

MODULE toy_ocn

  USE mpi
  USE yac
  USE yac_utils, ONLY : yac_test_gulfstream_c
  USE toy_common, ONLY : read_icon_grid, nsteps, define_fields, &
                         send_field, receive_field, &
                         max_char_length

  IMPLICIT NONE

  PRIVATE

  INTEGER :: t, ierror

  ! Basic string paramters
  CHARACTER(LEN=max_char_length), PARAMETER :: yaml_filename = "input/coupling.yaml"
  CHARACTER(LEN=max_char_length), PARAMETER :: grid_filename = "grids/icon_grid_0036_R02B04_O.nc"
  CHARACTER(LEN=max_char_length), PARAMETER :: comp_name = "ocn_comp"
  CHARACTER(LEN=max_char_length), PARAMETER :: grid_name = "ocn_grid"

  ! IDs and communicator generated by YAC
  INTEGER :: comp_id
  INTEGER :: comp_comm = MPI_COMM_WORLD
  INTEGER :: comp_rank
  INTEGER :: grid_id
  INTEGER :: cell_point_id
  INTEGER :: field_taux_id, field_tauy_id
  INTEGER :: field_sfwflx_id
  INTEGER :: field_sftemp_id
  INTEGER :: field_thflx_id
  INTEGER :: field_iceatm_id, field_iceoce_id
  INTEGER :: field_sst_id
  INTEGER :: field_oceanu_id, field_oceanv_id
  integer :: i
  
  ! Basic decomposed grid information
  INTEGER(kind = 4)             :: num_vertices_lon, num_vertices_lat
  INTEGER(kind = 4)             :: num_vertices, num_cells, num_vertices_per_cell
  INTEGER, ALLOCATABLE          :: cell_to_vertex(:,:)

  DOUBLE PRECISION, ALLOCATABLE :: x_vertices(:)
  DOUBLE PRECISION, ALLOCATABLE :: y_vertices(:)
  DOUBLE PRECISION, ALLOCATABLE :: x_cells(:)
  DOUBLE PRECISION, ALLOCATABLE :: y_cells(:)

  INTEGER, ALLOCATABLE          :: cell_sea_land_mask(:)
  INTEGER, ALLOCATABLE          :: global_cell_id(:)

  ! Buffer for field data
  DOUBLE PRECISION, DIMENSION(:,:), ALLOCATABLE :: &
       taux, tauy, sfwflx, sftemp, thflx, iceatm, sst, oceanu, oceanv, iceoce

  PUBLIC :: main_ocn

CONTAINS

  SUBROUTINE main_ocn(comm)
    INTEGER, INTENT(IN) :: comm
    character(len = 200) :: file = 'grids/GEIA_MPIC1.0_X_bioland_NH3_2000-2000.nc' 

    comp_comm = comm

    ! Read coupling configuration file
    CALL yac_fread_config_yaml(yaml_filename)

    ! Define local component
    CALL yac_fdef_comp(comp_name, comp_id)

    ! Retrieve communicator for OCN component
    CALL yac_fget_comp_comm(comp_id, comp_comm)

    CALL MPI_Comm_rank(comp_comm, comp_rank, ierror)

    ! Read the grid and distribute it among all OCN processes
!    CALL read_icon_grid( &
!         grid_filename, comp_comm, cell_to_vertex, x_vertices, &
!         y_vertices, x_cells, y_cells, cell_sea_land_mask, global_cell_id)
!    num_vertices = SIZE(x_vertices)
!    num_cells = SIZE(x_cells)
!    num_vertices_per_cell = SIZE(cell_to_vertex, 1)

    ! Define local part of the grid
    call read_grid_from_netcdf(trim(file), num_vertices_lon, num_vertices_lat, num_vertices, num_cells)
    num_vertices_per_cell = 4
    write(*, *) "Values"
    write(*, *) num_vertices_lon, num_vertices_lat, num_vertices, num_cells, num_vertices_per_cell

    ! Allocate and fill the vertex arrays (for longitude and lattitude
    allocate(x_vertices(num_vertices_lon))
    allocate(y_vertices(num_vertices_lat))
    do i = 1, num_vertices_lon
       x_vertices(i) = -180.0 + (i - 1) * 1.0
    end do
    
    do i = 1, num_vertices_lat
       y_vertices(i) = -90.0 + (i - 1) * 1.0
    end do
    
    write(*, *) x_vertices
    write(*, *) y_vertices

    ! Allocate and fill the arry cell_to_vertex with the vertices of the elements
    allocate(cell_to_vertex(num_cells, 4))
    write(*, *) "Size: ", size(cell_to_vertex, dim=1)

    ! Create the numerbing of the cells (column wise; from bottom totop)
    do i = 1, num_cells
       cell_to_vertex(i, 1) = 1 + (i / num_vertices_lat) * num_vertices_lat
       cell_to_vertex(i, 2) = 2 + (i / num_vertices_lat) * num_vertices_lat
       cell_to_vertex(i, 3) = 6 + (i / num_vertices_lat) * num_vertices_lat
       cell_to_vertex(i, 4) = 7 + (i / num_vertices_lat) * num_vertices_lat
       write(*, *) "element number: ", i, ", num_cells: ", num_cells
       write(*, *) cell_to_vertex(i, 1), cell_to_vertex(i, 2), cell_to_vertex(i, 3), cell_to_vertex(i, 4)
    end do

!     write(*, *) "Size: ", size(cell_to_vertex)
!    do i = 1, num_cells
!      write(*, *) cell_to_vertex(i, 1), cell_to_vertex(i, 2), cell_to_vertex(i, 3), cell_to_vertex(i, 4)
!    end do  

    CALL yac_fdef_grid ( &
         grid_name, num_vertices, num_cells, num_vertices_per_cell, &
         x_vertices, y_vertices, cell_to_vertex, grid_id )

    ! Define location of the actual data (on cell centers)
    CALL yac_fdef_points ( &
         grid_id, num_cells, YAC_LOCATION_CELL, &
         x_cells, y_cells, cell_point_id )

    ! Set global cell ids
    CALL yac_fset_global_index(global_cell_id, YAC_LOCATION_CELL, grid_id)

    ! Set mask for cell centers
    CALL yac_fset_mask(cell_sea_land_mask < 0, cell_point_id)

    ! Define fields
    CALL define_fields( &
         comp_id, cell_point_id, field_taux_id, field_tauy_id, field_sfwflx_id, &
         field_sftemp_id, field_thflx_id, field_iceatm_id, field_sst_id, &
         field_oceanu_id, field_oceanv_id, field_iceoce_id)

    ! Complete definitions and compute interpolations
    CALL yac_fenddef()

    ! Initialise fields
    CALL init_fields()

    ! Execute model time loop
    DO t = 1, nsteps

       ! Simulate ocn timestep
       CALL sim_ocn_timestep(t)

       ! Exchange data with atm component
       CALL couple_to_atm()

    END DO ! time loop

  END SUBROUTINE main_ocn

  SUBROUTINE init_fields()

    INTEGER :: i

    ! Allocate output field buffers
    ALLOCATE( &
      sst(num_cells, 1), oceanu(num_cells, 1), oceanv(num_cells, 1), &
      iceoce(num_cells, 5))

    ! Initialise output field buffer with dummy data
    DO i = 1, num_cells
      sst(i,1) = yac_test_gulfstream_c(x_cells(i), y_cells(i))
    END DO
    oceanu(:,1) = 120.0d0
    oceanv(:,1) = 130.0d0
    iceoce(:,1) = 140.1d0
    iceoce(:,2) = 140.2d0
    iceoce(:,3) = 140.3d0
    iceoce(:,4) = 140.4d0
    iceoce(:,5) = 140.5d0

    ! Allocate input field buffers
    ALLOCATE( &
      taux(num_cells, 2), tauy(num_cells, 2), sfwflx(num_cells, 3), &
      sftemp(num_cells, 1), thflx(num_cells, 4), iceatm(num_cells, 4))

    ! Initialise input field buffer with zero
    taux(:,1) = 0.0d0
    taux(:,2) = 0.0d0
    tauy(:,1) = 0.0d0
    tauy(:,2) = 0.0d0
    sfwflx(:,1) = 0.0d0
    sfwflx(:,2) = 0.0d0
    sfwflx(:,3) = 0.0d0
    sftemp(:,1) = 0.0d0
    thflx(:,1) = 0.0d0
    thflx(:,2) = 0.0d0
    thflx(:,3) = 0.0d0
    thflx(:,4) = 0.0d0
    iceatm(:,1) = 0.0d0
    iceatm(:,2) = 0.0d0
    iceatm(:,3) = 0.0d0
    iceatm(:,4) = 0.0d0

  END SUBROUTINE init_fields

  SUBROUTINE sim_ocn_timestep(timestep)

    INTEGER, INTENT(IN) :: timestep

    ! Do an ocean timestep

    ! ...

    CALL MPI_Barrier(comp_comm, ierror)
    IF (comp_rank == 0) &
      PRINT "('--- ',A3,' timestep ',I2,' ---')", comp_name, timestep

  END SUBROUTINE sim_ocn_timestep

  SUBROUTINE couple_to_atm()

    ! --------------------
    ! Send fields to ocean
    ! --------------------

    ! Send sea surface temperature
    CALL send_field(field_sst_id, sst)

    ! Send zonal velocity
    CALL send_field(field_oceanu_id, oceanu)

    ! Send meridional velocity
    CALL send_field(field_oceanv_id, oceanv)

    ! Send Ice thickness, concentration, T1 and T2
    CALL send_field(field_iceoce_id, iceoce)

    ! -------------------------
    ! Receive fields from ocean
    ! -------------------------

    ! Recieve meridional wind stress
    CALL receive_field(comp_name, field_taux_id, taux)

    ! Recieve zonal  wind stress
    CALL receive_field(comp_name, field_tauy_id, tauy)

    ! Recieve surface fresh water flux
    CALL receive_field(comp_name, field_sfwflx_id, sfwflx)

    ! Recieve surface temperature
    CALL receive_field(comp_name, field_sftemp_id, sftemp)

    ! Recieve total heat flux
    CALL receive_field(comp_name, field_thflx_id, thflx)

    ! Recieve ice temperatures and melt potential
    CALL receive_field(comp_name, field_iceatm_id, iceatm)

  END SUBROUTINE couple_to_atm


    ! ===================== subroutine read_grid_from_netcdf =========================
  subroutine read_grid_from_netcdf(filename, num_vertices_lon, num_vertices_lat, num_vertices, num_cells)
    use netcdf
    implicit none

    character(len = *) :: filename
    character(len = 200) :: name
    character(len = 200), allocatable, dimension(:) :: varnames

    integer :: status, xtype, ncid, ndim, nvar, natt, natts
    integer :: len, k, ndims, k_un
    integer(kind = 4), intent(out) :: num_vertices_lon, num_vertices_lat, num_vertices
    integer(kind = 4), intent(out) :: num_cells
    integer(kind = 4), allocatable, dimension(:) :: dimids
    integer(kind = 4), allocatable, dimension(:) :: varids, vardims, vardatatype, varnatts, vardimids

    ! Open netCDF file
    write(*, *)
    write(*, *) "----- Open netCDF file -----"
    status = nf90_open(trim(filename), nf90_nowrite, ncid)
    if (status /= nf90_noerr) then
       write(*, *) 'could not open::', filename
       write(*, *) status
       stop 'parse_nc [1]'
    endif

    ! Inquiry of the nuber of variables, etc
    write(*, *)
    write(*, *) "----- Inquire number of dimensions, variables, etc -----"
    status = nf90_inquire(ncid, ndim, nvar, natt, k_un)
    if (status /= nf90_noerr) then
       write(*, *) 'nf90_inquire error'
       write(*, *) status
       stop 'parse_nc [1]'
    endif
    write(*, *) 'ndim, nvar, natt, k_un:', ndim, nvar, natt, k_un

    ! Output of the dimensions
    allocate(varids(nvar), vardims(nvar), vardatatype(nvar), varnames(nvar), varnatts(nvar), vardimids(nvar))

    write(*, *)
    write(*, *) "----- Dimensions -----"
    do k = 1, ndim
       status = nf90_inquire_dimension(ncid, k, name, len)
       if (status /= nf90_noerr) then
          write(*, *) "n90_inquire_dimension error"
          write(*, *) status
          stop 'parse_nc [1]'
       endif

       ! read number vertices in longitude directions (vertex is midpoint of cell)
       if(index(trim(name), 'lon') /= 0) then
          num_vertices_lon = len 
       end if

       ! read number vertices in lattitude directions (vertex is midpoint of cell)
       if(index(trim(name), 'lat') /= 0) then
          num_vertices_lat = len
       end if
    enddo
    
    ! Number of cells is product of vertices in lat and lon direction
    num_cells = num_vertices_lon * num_vertices_lat
    write(*, *) "num_cells: ", num_cells

    ! Now correction to number of vertices if vertex is on a real node and not in cell center
    num_vertices_lon = num_vertices_lon + 1
    num_vertices_lat = num_vertices_lat + 1 
    write(*, *) num_vertices_lon, num_vertices_lat

    num_vertices = num_vertices_lon * num_vertices_lat
    write(*, *) "num_vertices: ", num_vertices
    !write(*, *) 'axes ', dimids(k), dimnames(k), dimlengths(k)

       ! Output of the variables
    write(*, *)
    write(*, *) "----- Variables -----"
    do k = 1, nvar
       name = ''
       status = nf90_inquire_variable(ncid, k, name, xtype, ndims, dimids, natts)
       if (status /= nf90_noerr) then
          write(*, *) "n90_inquire_variable error"
          write(*, *) status
          stop 'parse_nc [1]'
       endif

       varids(k) = k
       varnames(k) = trim(name)
       vardatatype(k) = xtype
       vardims(k) = ndims
       varnatts(k) = natts
       !     vardimids(k) = dimids
       write(*,'(a,I3,a,a,a,I3,a,I3,a,I3)') "variable id", varids(k), ", varname: ", trim(varnames(k)), ", vardatatype: ", &
            vardatatype(k), ", vardim: ", vardims(k), ", variable number of attributes: ", varnatts(k)

       if(index(varnames(k), 'lon') /= 0) then
          write(*, *) "lon found"
          !        l1 = dimlengths(dimids(1))
          !        l2 = dimlengths(dimids(2))
          !        l3 = dimlengths(dimids(3))
          !        allocate( data(l1, l2, l3) )
          !        status = nf90_get_var( ncid, k, data, (/ 1,1,1 /) )
       end if

       if(index(varnames(k), 'lat') /= 0) then
          write(*, *) "lat found"
       end if

       if(natts > 0) then
          write(* ,*) '==== data attributes ===='
          call parse_atlist(ncid, natts, k)
          write(*, *)
       endif

    enddo

    ! Output of the attributes
    write(*, *) "----- Attributes -----"
    if(natt > 0) then
       write(*, *) '==== global attributes ===='
       call parse_atlist(ncid, natt, nf90_global)
       write(*, *)
    endif

  end subroutine read_grid_from_netcdf

  !=============================================================================
  subroutine parse_atlist( ncid, natt, varid )
    use netcdf

    character (len = 256) :: c1d
    character (len = 128) :: name
    integer(kind = 4) :: ncid, natt, varid
    integer(kind = 4) :: xtype, len
    integer(kind = 4) :: k, kk, status
    real(kind = 8) :: rval

    integer(kind = 4), allocatable, dimension(:) :: ivals
    real(kind = 8), allocatable, dimension(:) :: rvals

    do k = 1, natt
       status = nf90_inq_attname(ncid, varid, k, name)
       status = nf90_inquire_attribute(ncid, varid, name, xtype, len, kk)

       if (xtype == NF90_CHAR) then
          if (len > 256) then
             write(*,*) 'truncating attribute to length:', 256
          endif
          status = nf90_get_att(ncid, varid, name, c1d )
          if (status /= 0) stop 'error calling get_att'
          write(*, '(a," ", a)') trim(name), trim(c1d)
       else if ( xtype == NF90_INT ) then
          if (len > 1) then
             allocate( ivals(len) )
             status = nf90_get_att(ncid, varid, name, ivals )
             if (status /= 0) stop "error calling get_att for integer array"
             write(*,'(a,3h:: ,(i6))') trim(name), ivals
             deallocate( ivals )
          else
             status = nf90_get_att(ncid, varid, name, ivals )
             if (status /= 0) stop 'error calling get_att for integer'
             write(6,'(a,3h:: ,(i6))') trim(name), ivals
          endif
       else if( xtype == NF90_FLOAT ) then
          if (len > 1) then
             allocate( rvals(len) )
             status = nf90_get_att(ncid, varid, name, rvals )
             if (status /= 0) stop 'error calling get_att for real array'
             write(*,'(a,3h:: ,(e12.5))') trim(name), rvals
             deallocate( rvals )
          else
             status = nf90_get_att(ncid, varid, name, rval )
             if(status /= 0) stop "error calling get_att for real"
             write(6, '(a,3h:: ,(e12.5))') trim(name), rval
          endif
       else if( xtype == NF90_DOUBLE ) then
          if (len > 1) then
             allocate( rvals(len) )
             status = nf90_get_att(ncid, varid, name, rvals )
             if (status /= 0) stop "error calling get_att for real array"
             write(*, '(a,3h:: ,(e12.5))') trim(name), rvals
             deallocate( rvals )
          else
             status = nf90_get_att(ncid, varid, name, rval )
             if(status /= 0) stop "error calling get_att for real"
             write(6, '(a,3h:: ,(e12.5))') trim(name), rval
          endif

       endif
    enddo

  end subroutine parse_atlist

END MODULE toy_ocn

PROGRAM main_ocn_program
  USE mpi
  USE yac
  USE toy_ocn, ONLY : main_ocn

  IMPLICIT NONE

  INTEGER :: ierror

  ! Initialise MPI
  CALL MPI_Init(ierror)

  ! Initialise the YAC
  CALL yac_finit()

  ! Run atmosphere model
  CALL main_ocn(MPI_COMM_WORLD)

  ! Finalise YAC
  CALL yac_ffinalize()

  ! Finalise MPI
  CALL MPI_Finalize(ierror)

END PROGRAM main_ocn_program
